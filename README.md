
# ğŸ§  Fine-Tuning GPT-2 with LoRA on Wikitext (Minimal Example)

This project demonstrates how to fine-tune a pre-trained [GPT-2](https://huggingface.co/gpt2) language model using [LoRA (Low-Rank Adaptation)](https://arxiv.org/abs/2106.09685) for parameter-efficient training. It uses the Hugging Face `transformers`, `datasets`, and `peft` libraries and runs comfortably in Google Colab.

---

## ğŸ“¦ Features

- ğŸ§  GPT-2 fine-tuning using [PEFT](https://github.com/huggingface/peft)
- âš¡ Lightweight training with LoRA
- ğŸ“ Trained on a small subset of the Wikitext-103 dataset
- âœ… Colab-compatible: ~10â€“15 mins on GPU
- ğŸ’¾ Final model is saved locally and ready for inference



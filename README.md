
# 🧠 Fine-Tuning GPT-2 with LoRA on Wikitext (Minimal Example)

This project demonstrates how to fine-tune a pre-trained [GPT-2](https://huggingface.co/gpt2) language model using [LoRA (Low-Rank Adaptation)](https://arxiv.org/abs/2106.09685) for parameter-efficient training. It uses the Hugging Face `transformers`, `datasets`, and `peft` libraries and runs comfortably in Google Colab.

---

## 📦 Features

- 🧠 GPT-2 fine-tuning using [PEFT](https://github.com/huggingface/peft)
- ⚡ Lightweight training with LoRA
- 📝 Trained on a small subset of the Wikitext-103 dataset
- ✅ Colab-compatible: ~10–15 mins on GPU
- 💾 Final model is saved locally and ready for inference


